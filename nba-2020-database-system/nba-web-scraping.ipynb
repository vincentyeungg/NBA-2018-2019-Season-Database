{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "base_url = \"https://www.basketball-reference.com\"\n",
    "url = f\"https://www.basketball-reference.com/leagues/NBA_{year}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "data = requests.get(url)\n",
    "\n",
    "# create directory\n",
    "dir_name = 'data'\n",
    "try:\n",
    "    os.makedirs(dir_name)\n",
    "except OSError as error:\n",
    "    print(f'Directory \"{dir_name}\" cannot be created')\n",
    "\n",
    "# download data and save into html file for data parsing, instead of downloading the entire page each time\n",
    "with open(f\"data/{year}.html\", \"w+\", encoding=\"cp437\", errors='ignore') as f:\n",
    "    f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{year}.html\") as f:\n",
    "    # read the file and store the data as a string obj\n",
    "    page = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize soup object to parse html data\n",
    "soup = BeautifulSoup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the abbreviated team names and store into a list (some abbreviated names are different on this site)\n",
    "nba_team_pages = []\n",
    "\n",
    "# find the appropriate tables with the team stats for every team in the NBA\n",
    "eastern_conf_table = soup.find(id=\"confs_standings_E\")\n",
    "western_conf_table = soup.find(id=\"confs_standings_W\")\n",
    "\n",
    "# eastern conference teams\n",
    "for a in eastern_conf_table.find_all('a'):\n",
    "    nba_team_pages.append(a['href'])\n",
    "    \n",
    "# western conference teams\n",
    "for a in western_conf_table.find_all('a'):\n",
    "    nba_team_pages.append(a['href'])\n",
    "    \n",
    "# team abbrs\n",
    "team_abbrs = []\n",
    "\n",
    "for abbr in nba_team_pages:\n",
    "    team_abbrs.append(abbr.split('/')[2])\n",
    "    \n",
    "# create directory\n",
    "parent_dir = dir_name\n",
    "dir_name = os.path.join(parent_dir, 'teams')\n",
    "\n",
    "try:\n",
    "    os.makedirs(dir_name)\n",
    "except OSError as error:\n",
    "    print(f'Directory \"{dir_name}\" cannot be created')\n",
    "\n",
    "# go through each page and save them to their corresponding html files\n",
    "for team_page, abbr_name in zip(nba_team_pages, team_abbrs):\n",
    "    data = requests.get(base_url + team_page)\n",
    "    \n",
    "    # download data and save into html file for data parsing, instead of downloading the entire page each time\n",
    "    with open(f\"{dir_name}/{abbr_name}.html\", \"w+\", encoding=\"cp437\", errors='ignore') as f:\n",
    "        f.write(data.text)\n",
    "\n",
    "# at this step, you will have the html files of all the individual teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atlanta Hawks'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each team should have a name, abbr name, record\n",
    "with open('data/teams/ATL.html') as f:\n",
    "    page = f.read()\n",
    "    \n",
    "    # initialize soup object to parse html data\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    # team name\n",
    "    team_name = soup.find(id=\"info\").find_all('span')[1].extract().text\n",
    "    \n",
    "    # record\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eastern_conf_2019_df = pd.read_html(str(eastern_conf_table))[0]\n",
    "# western_conf_2019_df = pd.read_html(str(western_conf_table))[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6184136c414f2ed1c234f4dbd1a92b7a98a1510cf556a32e9118757207ffca48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
