{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "base_url = \"https://www.basketball-reference.com\"\n",
    "url = f\"https://www.basketball-reference.com/leagues/NBA_{year}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Obtaining data for 'Teams' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - create directory to store downloaded files, obtain data for each time, and save to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "data = requests.get(url)\n",
    "\n",
    "# create directory\n",
    "dir_name = 'data'\n",
    "try:\n",
    "    os.makedirs(dir_name)\n",
    "except OSError as error:\n",
    "    print(f'Directory \"{dir_name}\" cannot be created')\n",
    "\n",
    "# download data and save into html file for data parsing, instead of downloading the entire page each time\n",
    "with open(f\"data/{year}.html\", \"w+\", encoding=\"cp437\", errors='ignore') as f:\n",
    "    f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/{year}.html\") as f:\n",
    "    # read the file and store the data as a string obj\n",
    "    page = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize soup object to parse html data\n",
    "soup = BeautifulSoup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the abbreviated team names and store into a list (some abbreviated names are different on this site)\n",
    "nba_team_pages = []\n",
    "\n",
    "# find the appropriate tables with the team stats for every team in the NBA\n",
    "eastern_conf_table = soup.find(id=\"confs_standings_E\")\n",
    "western_conf_table = soup.find(id=\"confs_standings_W\")\n",
    "\n",
    "# eastern conference teams\n",
    "for a in eastern_conf_table.find_all('a'):\n",
    "    nba_team_pages.append(a['href'])\n",
    "    \n",
    "# western conference teams\n",
    "for a in western_conf_table.find_all('a'):\n",
    "    nba_team_pages.append(a['href'])\n",
    "    \n",
    "# team abbrs\n",
    "team_abbrs = []\n",
    "\n",
    "for abbr in nba_team_pages:\n",
    "    team_abbrs.append(abbr.split('/')[2])\n",
    "    \n",
    "# create directory\n",
    "parent_dir = dir_name\n",
    "dir_name = os.path.join(parent_dir, 'teams')\n",
    "\n",
    "try:\n",
    "    os.makedirs(dir_name)\n",
    "except OSError as error:\n",
    "    print(f'Directory \"{dir_name}\" cannot be created')\n",
    "\n",
    "# go through each page and save them to their corresponding html files\n",
    "for team_page, abbr_name in zip(nba_team_pages, team_abbrs):\n",
    "    data = requests.get(base_url + team_page)\n",
    "    \n",
    "    # download data and save into html file for data parsing, instead of downloading the entire page each time\n",
    "    with open(f\"{dir_name}/{abbr_name}.html\", \"w+\", encoding=\"cp437\", errors='ignore') as f:\n",
    "        f.write(data.text)\n",
    "\n",
    "# at this step, you will have the html files of all the individual teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_abbrs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "teams_df = pd.DataFrame(columns=['Name', 'Abbreviated Name', 'Arena', 'Wins', 'Losses', 'W-L'])\n",
    "\n",
    "for idx, team in enumerate(team_abbrs, start=1):\n",
    "    # each team should have a name, abbr name, wins, losses, location\n",
    "    with open(f'data/teams/{team}.html', encoding=\"cp437\", errors='ignore') as f:\n",
    "        page = f.read()\n",
    "        \n",
    "        # initialize soup object to parse html data\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        \n",
    "        # team name\n",
    "        team_name = soup.find(id=\"info\").find_all('span')[1].extract().text\n",
    "        \n",
    "        # wins/losses\n",
    "        wins_and_losses = soup.find(id=\"info\").select_one('div[data-template=\"Partials/Teams/Summary\"]').find('p').extract().text.replace(\" \", \"\").replace(\"\\n\", \"\").split(\",\")[0].split(\":\")[1]\n",
    "        wins = wins_and_losses.split(\"-\")[0]\n",
    "        losses = wins_and_losses.split(\"-\")[1]\n",
    "        \n",
    "        # location i.e. arena\n",
    "        # filter string to find the arena name\n",
    "        locations = soup.find(id=\"info\").select_one('div[data-template=\"Partials/Teams/Summary\"]').find_all('p')\n",
    "        pattern = 'Attendance'\n",
    "        location = None\n",
    "        match = None\n",
    "        \n",
    "        # find the tag with the arena\n",
    "        for loc in locations:\n",
    "            match = (re.search(pattern, loc.text))\n",
    "            if match:\n",
    "                location = loc.text\n",
    "                break\n",
    "        \n",
    "        # # obtain only the arena name, and filter further\n",
    "        # location = location[:match.start()].replace(\"\\n\", \"\").replace(\" \", \"\").split(\":\")[-1]\n",
    "        location = location[:match.start()].replace(\"\\n\", \"\").split(\":\")[-1].strip()\n",
    "        \n",
    "        teams_df.loc[idx] = [team_name, team, location, wins, losses, wins_and_losses]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory \"csv_files\" cannot be created\n"
     ]
    }
   ],
   "source": [
    "# create directory\n",
    "dir_name = 'csv_files'\n",
    "\n",
    "try:\n",
    "    os.makedirs(dir_name)\n",
    "except OSError as error:\n",
    "    print(f'Directory \"{dir_name}\" cannot be created')\n",
    "    \n",
    "teams_df.to_csv('csv_files/teams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain team stats data for each team, filter data, convert to dataframe, and save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "teams_stats_df = pd.DataFrame(columns=['Name', 'Field Goal %', 'Three-Point Field Goal %', 'Free Throw %', 'Turnovers Per Game', 'Offensive Rebounds Per Game', 'Defensive Rebounds Per Game', 'Assists Per Game', 'Steals Per Game', 'Blocks Per Game', 'Points Per Game'])\n",
    "\n",
    "for idx, team in enumerate(team_abbrs, start=1):\n",
    "    # each team should have a name, abbr name, wins, losses, location\n",
    "    with open(f'data/teams/{team}.html', encoding=\"cp437\", errors='ignore') as f:\n",
    "        page = f.read()\n",
    "        \n",
    "        # initialize soup object to parse html data\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        \n",
    "        # need to use Selenium to scrape dynamic content\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6184136c414f2ed1c234f4dbd1a92b7a98a1510cf556a32e9118757207ffca48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
